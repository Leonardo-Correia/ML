{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import grad, jit, vmap\n",
    "import flax \n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib.pyplot as plt\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Residual Network (AResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class ResBlockPostActivationPeriodic1d(eqx.Module):\n",
    "    conv_1: eqx.nn.Conv1d # This is the first convolutional layer\n",
    "    conv_2: eqx.nn.Conv1d # This is the second convolutional layer\n",
    "    activation: Callable # This is the activation function, and the Callable type is a function that returns a value\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int,\n",
    "        activation: Callable,\n",
    "        *, # This is a separator to indicate that the following arguments are keyword-only\n",
    "        key,\n",
    "    ):\n",
    "        c_1_key, c_2_key = jax.random.split(key)\n",
    "\n",
    "        # Requires an up-to-dat version of Equinox\n",
    "        self.conv_1 = eqx.nn.Conv1d( # This is the first convolutional layer\n",
    "            hidden_channels, # This is the number of input channels\n",
    "            hidden_channels, # This is the number of output channels\n",
    "            kernel_size=2, # This is the kernel size, it means that the convolutional layer will use 3 points to make the convolution\n",
    "            padding=\"SAME\", # This is the padding, it means that the convolutional layer will use the same size of the input\n",
    "            padding_mode=\"CIRCULAR\", # This is the padding mode, it means that the convolutional layer will use the circular padding\n",
    "            key=c_1_key \n",
    "        )\n",
    "        self.conv_2 = eqx.nn.Conv1d(\n",
    "            hidden_channels,\n",
    "            hidden_channels,\n",
    "            kernel_size=2,\n",
    "            padding=\"SAME\",\n",
    "            padding_mode=\"CIRCULAR\",\n",
    "            key=c_2_key\n",
    "        )\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        x_skip_1 = x\n",
    "        x = self.conv_1(x)\n",
    "        x = x + x_skip_1 # Residual connection\n",
    "        x = self.activation(x)\n",
    "        x_skip_2 = x\n",
    "        x = self.conv_2(x)\n",
    "        x = x + x_skip_2 # Residual connection\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.conv_1.weight,\n",
    "                self.conv_1.bias, \n",
    "                self.conv_2.weight, \n",
    "                self.conv_2.bias]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetPeriodic1d(eqx.Module):\n",
    "    lifting: eqx.nn.Conv1d # This is the first layer, the lifting layer that maps the input to the hidden space of the network with upper dimension\n",
    "    blocks: list[ResBlockPostActivationPeriodic1d] # This is the list of the ResNet blocks that will be used in the network.\n",
    "    projection: eqx.nn.Conv1d # This is another 1D convolutional layer with kernel size 1, which projects the final transformed data back to a lower-dimensional space.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        num_blocks,\n",
    "        activation,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        l_key, *block_keys, p_key = jax.random.split(key, num_blocks+2)\n",
    "\n",
    "        self.lifting = eqx.nn.Conv1d(1, hidden_channels, kernel_size=1, key=l_key)\n",
    "        self.blocks = [\n",
    "            ResBlockPostActivationPeriodic1d(hidden_channels, activation, key=k)\n",
    "            for k in block_keys\n",
    "        ]\n",
    "        self.projection = eqx.nn.Conv1d(hidden_channels, 1, kernel_size=1, key=p_key)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.lifting(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_params_tree(self):\n",
    "        # Retorna os par√¢metros das camadas\n",
    "        lifting_params = [\n",
    "            self.lifting.weight,\n",
    "            self.lifting.bias\n",
    "        ]\n",
    "\n",
    "        block_params = [block.get_params() for block in self.blocks]\n",
    "        \n",
    "        projection_params = [\n",
    "            self.projection.weight,\n",
    "            self.projection.bias\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            lifting_params,\n",
    "            block_params,\n",
    "            projection_params\n",
    "        ]\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        # Update lifting layer\n",
    "        self = eqx.tree_at(lambda m: m.lifting.weight, self, params[0][0])\n",
    "        self = eqx.tree_at(lambda m: m.lifting.bias, self, params[0][1])\n",
    "\n",
    "        # Update ResBlocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_1.weight, self, params[1][i][0])\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_1.bias, self, params[1][i][1])\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_2.weight, self, params[1][i][2])\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_2.bias, self, params[1][i][3])\n",
    "\n",
    "        # Update projection layer\n",
    "        self = eqx.tree_at(lambda m: m.projection.weight, self, params[2][0])\n",
    "        self = eqx.tree_at(lambda m: m.projection.bias, self, params[2][1])\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum([p.size for p in jax.tree.flatten(self.get_params_tree())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing get_params_tree() and set_params() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 1, 1), (1, 1)], [[(1, 1, 2), (1, 1), (1, 1, 2), (1, 1)]], [(1, 1, 1), (1, 1)]]\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede = ResNetPeriodic1d(1, 1, jax.nn.relu, key=jax.random.PRNGKey(1))\n",
    "# Ravel the parameters into a flat vector for manipulation\n",
    "param_list = (rede.get_params_tree())\n",
    "tree, tree_map = ravel_pytree(param_list)\n",
    "print(jax.tree_map(lambda x: x.shape, param_list))\n",
    "\n",
    "print(tree.shape[0])\n",
    "(param_list)\n",
    "\n",
    "rede.get_num_params()\n",
    "# key = jax.random.PRNGKey(0)\n",
    "# random_tree = jax.random.uniform(key, shape=tree.shape)\n",
    "# random_tree = tree_map(random_tree)\n",
    "\n",
    "# print(random_tree)\n",
    "\n",
    "# # Set new random parameters using the modified set_params method\n",
    "# rede = rede.set_params(random_tree)\n",
    "\n",
    "# # Check updated parameters\n",
    "# print(rede.get_params_tree())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a MultiLayer Perceptron (MLP) model in JAX\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 200\n",
    "LAYERS = [1,1]\n",
    "ACTIVATION = jax.nn.relu\n",
    "LEARNING_RATE = 1e-3\n",
    "N_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNet(eqx.Module):\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_size = [],\n",
    "        last_layer_size = None,\n",
    "        key = None,\n",
    "    ):\n",
    "        self.layers = []\n",
    "        for (fan_in, fan_out) in zip(layers_size[:-1], layers_size[1:]):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            self.layers.append(eqx.nn.Linear(fan_in, fan_out, key=subkey, use_bias=True))\n",
    "        \n",
    "        self.layers.append(eqx.nn.Linear(layers_size[-1], last_layer_size, key=subkey, use_bias=True))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]: # Iterate over all layers except the last one\n",
    "            x = jax.nn.relu(layer(x))\n",
    "            # the last layer must not have an activation function in addition the output must have the last_layer_size shape\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [[layer.weight] + [layer.bias] for layer in self.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing get_params() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Array([[0.5729215]], dtype=float32), Array([-0.37414527], dtype=float32)],\n",
       " [Array([[ 0.4417026],\n",
       "         [-0.981369 ]], dtype=float32),\n",
       "  Array([ 0.73462176, -0.4050598 ], dtype=float32)]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "modelHN = HyperNet(LAYERS, last_layer_size= 2, key=key)\n",
    "\n",
    "modelHN.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class that inherits from the HyperNet and ResNetPeriodic1d\n",
    "\n",
    "class HyperResidualRNN(eqx.Module):\n",
    "    hypernet: HyperNet\n",
    "    targetnet: ResNetPeriodic1d\n",
    "    def __init__(self, hypernet, targetnet):\n",
    "        self.hypernet = hypernet\n",
    "        self.targetnet = targetnet\n",
    "\n",
    "        # Get the parameters for the targetnet\n",
    "    def get_unravel(self):\n",
    "        targetnet_params = self.targetnet.get_params_tree() #TODO removing the last layer\n",
    "        # Get the unravelling function for the targetnet\n",
    "        _,unravel = ravel_pytree(targetnet_params)\n",
    "        return unravel\n",
    "\n",
    "    def __call__(self, input_target, input_hyper,unravel_fn):\n",
    "        # Calculate the output of the hypernet\n",
    "        input_hyper = self.hypernet(input_hyper) # input in the hypernet to the output of the hypernet with the vector of targetnet parameters\n",
    "\n",
    "        # Unravel the parameters\n",
    "        targetnet_params = unravel_fn(input_hyper) # transform the output of the hypernet into the parameters of the targetnet\n",
    "\n",
    "        # Set the parameters of the targetnet\n",
    "        self.targetnet.set_params(targetnet_params) # set the parameters of the targetnet with the output of the hypernet\n",
    "\n",
    "        input_target = self.targetnet(input_target) \n",
    "\n",
    "        return input_target\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ScaleByAdamState(count=Array(0, dtype=int32), mu=HyperResidualRNN(\n",
       "   hypernet=HyperNet(\n",
       "     layers=[\n",
       "       Linear(\n",
       "         weight=f32[1,1],\n",
       "         bias=f32[1],\n",
       "         in_features=1,\n",
       "         out_features=1,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[16,1],\n",
       "         bias=f32[16],\n",
       "         in_features=1,\n",
       "         out_features=16,\n",
       "         use_bias=True\n",
       "       )\n",
       "     ]\n",
       "   ),\n",
       "   targetnet=ResNetPeriodic1d(\n",
       "     lifting=Conv1d(\n",
       "       num_spatial_dims=1,\n",
       "       weight=f32[1,1,1],\n",
       "       bias=f32[1,1],\n",
       "       in_channels=1,\n",
       "       out_channels=1,\n",
       "       kernel_size=(1,),\n",
       "       stride=(1,),\n",
       "       padding=((0, 0),),\n",
       "       dilation=(1,),\n",
       "       groups=1,\n",
       "       use_bias=True,\n",
       "       padding_mode='ZEROS'\n",
       "     ),\n",
       "     blocks=[\n",
       "       ResBlockPostActivationPeriodic1d(\n",
       "         conv_1=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         conv_2=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         activation=None\n",
       "       ),\n",
       "       ResBlockPostActivationPeriodic1d(\n",
       "         conv_1=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         conv_2=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         activation=None\n",
       "       )\n",
       "     ],\n",
       "     projection=Conv1d(\n",
       "       num_spatial_dims=1,\n",
       "       weight=f32[1,1,1],\n",
       "       bias=f32[1,1],\n",
       "       in_channels=1,\n",
       "       out_channels=1,\n",
       "       kernel_size=(1,),\n",
       "       stride=(1,),\n",
       "       padding=((0, 0),),\n",
       "       dilation=(1,),\n",
       "       groups=1,\n",
       "       use_bias=True,\n",
       "       padding_mode='ZEROS'\n",
       "     )\n",
       "   )\n",
       " ), nu=HyperResidualRNN(\n",
       "   hypernet=HyperNet(\n",
       "     layers=[\n",
       "       Linear(\n",
       "         weight=f32[1,1],\n",
       "         bias=f32[1],\n",
       "         in_features=1,\n",
       "         out_features=1,\n",
       "         use_bias=True\n",
       "       ),\n",
       "       Linear(\n",
       "         weight=f32[16,1],\n",
       "         bias=f32[16],\n",
       "         in_features=1,\n",
       "         out_features=16,\n",
       "         use_bias=True\n",
       "       )\n",
       "     ]\n",
       "   ),\n",
       "   targetnet=ResNetPeriodic1d(\n",
       "     lifting=Conv1d(\n",
       "       num_spatial_dims=1,\n",
       "       weight=f32[1,1,1],\n",
       "       bias=f32[1,1],\n",
       "       in_channels=1,\n",
       "       out_channels=1,\n",
       "       kernel_size=(1,),\n",
       "       stride=(1,),\n",
       "       padding=((0, 0),),\n",
       "       dilation=(1,),\n",
       "       groups=1,\n",
       "       use_bias=True,\n",
       "       padding_mode='ZEROS'\n",
       "     ),\n",
       "     blocks=[\n",
       "       ResBlockPostActivationPeriodic1d(\n",
       "         conv_1=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         conv_2=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         activation=None\n",
       "       ),\n",
       "       ResBlockPostActivationPeriodic1d(\n",
       "         conv_1=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         conv_2=Conv1d(\n",
       "           num_spatial_dims=1,\n",
       "           weight=f32[1,1,2],\n",
       "           bias=f32[1,1],\n",
       "           in_channels=1,\n",
       "           out_channels=1,\n",
       "           kernel_size=(2,),\n",
       "           stride=(1,),\n",
       "           padding='SAME',\n",
       "           dilation=(1,),\n",
       "           groups=1,\n",
       "           use_bias=True,\n",
       "           padding_mode='CIRCULAR'\n",
       "         ),\n",
       "         activation=None\n",
       "       )\n",
       "     ],\n",
       "     projection=Conv1d(\n",
       "       num_spatial_dims=1,\n",
       "       weight=f32[1,1,1],\n",
       "       bias=f32[1,1],\n",
       "       in_channels=1,\n",
       "       out_channels=1,\n",
       "       kernel_size=(1,),\n",
       "       stride=(1,),\n",
       "       padding=((0, 0),),\n",
       "       dilation=(1,),\n",
       "       groups=1,\n",
       "       use_bias=True,\n",
       "       padding_mode='ZEROS'\n",
       "     )\n",
       "   )\n",
       " )),\n",
       " EmptyState())"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TargetNet\n",
    "targetnet = ResNetPeriodic1d(1, 2, jax.nn.relu, key=jax.random.PRNGKey(1))\n",
    "targetnet_num_params = targetnet.get_num_params()\n",
    "\n",
    "# Create the HyperNet\n",
    "hypernet = HyperNet(LAYERS,last_layer_size = targetnet_num_params, key=key, activation=ACTIVATION)\n",
    "\n",
    "\n",
    "ravel_pytree(targetnet.get_params_tree())\n",
    "\n",
    "# Create the HyperResidualRNN\n",
    "model = HyperResidualRNN(hypernet, targetnet)\n",
    "\n",
    "unravel = model.get_unravel()\n",
    "\n",
    "optimizer = optax.adam(LEARNING_RATE)\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "# Creating the loss function\n",
    "@jax.jit\n",
    "def loss_fn(model, batch):\n",
    "    inputs, targets = batch\n",
    "    input_target, input_hyper = inputs\n",
    "    predictions = model(input_target, input_hyper, unravel)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Fun√ß√£o de atualiza√ß√£o do modelo\n",
    "@eqx.filter_jit\n",
    "def step_fn(model, state, batch):\n",
    "    loss, grad = eqx.filter_value_and_grad(loss_fn)(model, batch)\n",
    "    updates, new_state = optimizer.update(grad, state, model)\n",
    "    new_model = eqx.apply_updates(model, updates)\n",
    "    return new_model, new_state, loss\n",
    "\n",
    "opt_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
