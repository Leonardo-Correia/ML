{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/t07mkwr55sqbw9ld5m6_dhvm0000gn/T/ipykernel_36863/1763599204.py:15: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import grad, jit, vmap\n",
    "import flax \n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib.pyplot as plt\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Residual Network (AResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class ResBlockPostActivationPeriodic1d(eqx.Module):\n",
    "    conv_1: eqx.nn.Conv1d # This is the first convolutional layer\n",
    "    conv_2: eqx.nn.Conv1d # This is the second convolutional layer\n",
    "    activation: Callable # This is the activation function, and the Callable type is a function that returns a value\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int,\n",
    "        activation: Callable,\n",
    "        *, # This is a separator to indicate that the following arguments are keyword-only\n",
    "        key,\n",
    "    ):\n",
    "        c_1_key, c_2_key = jax.random.split(key)\n",
    "\n",
    "        # Requires an up-to-dat version of Equinox\n",
    "        self.conv_1 = eqx.nn.Conv1d( # This is the first convolutional layer\n",
    "            hidden_channels, # This is the number of input channels\n",
    "            hidden_channels, # This is the number of output channels\n",
    "            kernel_size=2, # This is the kernel size, it means that the convolutional layer will use 3 points to make the convolution\n",
    "            padding=\"SAME\", # This is the padding, it means that the convolutional layer will use the same size of the input\n",
    "            padding_mode=\"CIRCULAR\", # This is the padding mode, it means that the convolutional layer will use the circular padding\n",
    "            key=c_1_key \n",
    "        )\n",
    "        self.conv_2 = eqx.nn.Conv1d(\n",
    "            hidden_channels,\n",
    "            hidden_channels,\n",
    "            kernel_size=2,\n",
    "            padding=\"SAME\",\n",
    "            padding_mode=\"CIRCULAR\",\n",
    "            key=c_2_key\n",
    "        )\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        x_skip_1 = x\n",
    "        x = self.conv_1(x)\n",
    "        x = x + x_skip_1 # Residual connection\n",
    "        x = self.activation(x)\n",
    "        x_skip_2 = x\n",
    "        x = self.conv_2(x)\n",
    "        x = x + x_skip_2 # Residual connection\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.conv_1.weight,\n",
    "                self.conv_1.bias, \n",
    "                self.conv_2.weight, \n",
    "                self.conv_2.bias]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetPeriodic1d(eqx.Module):\n",
    "    lifting: eqx.nn.Conv1d # This is the first layer, the lifting layer that maps the input to the hidden space of the network with upper dimension\n",
    "    blocks: list[ResBlockPostActivationPeriodic1d] # This is the list of the ResNet blocks that will be used in the network.\n",
    "    projection: eqx.nn.Conv1d # This is another 1D convolutional layer with kernel size 1, which projects the final transformed data back to a lower-dimensional space.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        num_blocks,\n",
    "        activation,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        l_key, *block_keys, p_key = jax.random.split(key, num_blocks+2)\n",
    "\n",
    "        self.lifting = eqx.nn.Conv1d(1, hidden_channels, kernel_size=1, key=l_key)\n",
    "        self.blocks = [\n",
    "            ResBlockPostActivationPeriodic1d(hidden_channels, activation, key=k)\n",
    "            for k in block_keys\n",
    "        ]\n",
    "        self.projection = eqx.nn.Conv1d(hidden_channels, 1, kernel_size=1, key=p_key)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.lifting(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_params_tree(self):\n",
    "        # Retorna os parâmetros das camadas\n",
    "        lifting_params = [\n",
    "            self.lifting.weight,\n",
    "            self.lifting.bias\n",
    "        ]\n",
    "\n",
    "        block_params = [block.get_params() for block in self.blocks]\n",
    "        \n",
    "        projection_params = [\n",
    "            self.projection.weight,\n",
    "            self.projection.bias\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            lifting_params,\n",
    "            block_params,\n",
    "            projection_params\n",
    "        ]\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        # Update lifting layer\n",
    "        self = eqx.tree_at(lambda m: m.lifting.weight, self, params[0][0])\n",
    "        self = eqx.tree_at(lambda m: m.lifting.bias, self, params[0][1])\n",
    "\n",
    "        # Update ResBlocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_1.weight, self, params[1][i][0])\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_1.bias, self, params[1][i][1])\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_2.weight, self, params[1][i][2])\n",
    "            self = eqx.tree_at(lambda m: m.blocks[i].conv_2.bias, self, params[1][i][3])\n",
    "\n",
    "        # Update projection layer\n",
    "        self = eqx.tree_at(lambda m: m.projection.weight, self, params[2][0])\n",
    "        self = eqx.tree_at(lambda m: m.projection.bias, self, params[2][1])\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing get_params_tree() and set_params() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Array([[[0.31059694]]], dtype=float32), Array([[0.5476253]], dtype=float32)],\n",
       " [[Array([[[-0.09132952, -0.01586188]]], dtype=float32),\n",
       "   Array([[0.3417482]], dtype=float32),\n",
       "   Array([[[ 0.05476376, -0.09840479]]], dtype=float32),\n",
       "   Array([[0.43600458]], dtype=float32)]],\n",
       " [Array([[[-0.19735742]]], dtype=float32),\n",
       "  Array([[0.8156264]], dtype=float32)]]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede = ResNetPeriodic1d(1, 1, jax.nn.relu, key=jax.random.PRNGKey(1))\n",
    "# Ravel the parameters into a flat vector for manipulation\n",
    "rede.get_params_tree()\n",
    "tree, tree_map = ravel_pytree(rede.get_params_tree())\n",
    "tree\n",
    "# key = jax.random.PRNGKey(0)\n",
    "# random_tree = jax.random.uniform(key, shape=tree.shape)\n",
    "# random_tree = tree_map(random_tree)\n",
    "\n",
    "# print(random_tree)\n",
    "\n",
    "# # Set new random parameters using the modified set_params method\n",
    "# rede = rede.set_params(random_tree)\n",
    "\n",
    "# # Check updated parameters\n",
    "# print(rede.get_params_tree())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a MultiLayer Perceptron (MLP) model in JAX\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 200\n",
    "LAYERS = [1, 2, 1]\n",
    "ACTIVATION = jax.nn.relu\n",
    "LEARNING_RATE = 1e-3\n",
    "N_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNet(eqx.Module):\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_size = [],\n",
    "        key = None,\n",
    "        activation = jax.nn.relu\n",
    "    ):\n",
    "        self.layers = []\n",
    "        for (fan_in, fan_out) in zip(layers_size[:-1], layers_size[1:]):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            self.layers.append(eqx.nn.Linear(fan_in, fan_out, key=subkey, use_bias=True))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]: # Iterate over all layers except the last one\n",
    "            x = ACTIVATION(layer(x))\n",
    "        return self.layers[-1](x) # Apply the last layer\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [layer.weight for layer in self.layers] + [layer.bias for layer in self.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing get_params() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[-0.19835997],\n",
       "        [-0.7369585 ]], dtype=float32),\n",
       " Array([[-0.3148051 ,  0.26949587]], dtype=float32),\n",
       " Array([0.69720936, 0.8496475 ], dtype=float32),\n",
       " Array([-0.09411155], dtype=float32)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelHN = HyperNet(LAYERS, key=key, activation=ACTIVATION)\n",
    "\n",
    "modelHN.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class that inherits from the HyperNet and ResNetPeriodic1d\n",
    "\n",
    "class HyperResidualRNN(eqx.Module):\n",
    "    hypernet: HyperNet\n",
    "    targetnet: ResNetPeriodic1d\n",
    "    def __init__(self, hypernet, targetnet):\n",
    "        self.hypernet = hypernet\n",
    "        self.targetnet = targetnet\n",
    "\n",
    "        # Get the parameters for the targetnet\n",
    "    def get_unravel(self):\n",
    "        targetnet_params = self.targetnet.get_params_tree() #TODO removing the last layer\n",
    "        # Get the unravelling function for the targetnet\n",
    "        _,unravel = ravel_pytree(targetnet_params)\n",
    "        return unravel\n",
    "\n",
    "    def __call__(self, input_target, input_hyper, unravel_function):\n",
    "        # Calculate the output of the hypernet\n",
    "        input_hyper = self.hypernet(input_hyper)\n",
    "\n",
    "        # Unravel the parameters\n",
    "        targetnet_params = unravel_function(input_hyper)\n",
    "\n",
    "        # Set the parameters of the targetnet\n",
    "        self.targetnet.set_params(targetnet_params)\n",
    "\n",
    "        input_target = self.targetnet(input_target)\n",
    "\n",
    "        return input_target\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HyperNet\n",
    "hypernet = HyperNet(LAYERS, key=key, activation=ACTIVATION)\n",
    "\n",
    "# Create the TargetNet\n",
    "targetnet = ResNetPeriodic1d(1, 2, jax.nn.relu, key=jax.random.PRNGKey(1))\n",
    "\n",
    "ravel_pytree(targetnet.get_params_tree())\n",
    "\n",
    "# Create the HyperResidualRNN\n",
    "model = HyperResidualRNN(hypernet, targetnet)\n",
    "\n",
    "unravel = model.get_unravel()\n",
    "\n",
    "optimizer = optax.adam(LEARNING_RATE)\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "# Creating the loss function\n",
    "@jax.jit\n",
    "def loss_fn(model, batch):\n",
    "    inputs, targets = batch\n",
    "    input_target, input_hyper = inputs\n",
    "    predictions = model(input_target, input_hyper, unravel)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Função de atualização do modelo\n",
    "@eqx.filter_jit\n",
    "def step_fn(model, state, batch):\n",
    "    loss, grad = eqx.filter_value_and_grad(loss_fn)(model, batch)\n",
    "    updates, new_state = optimizer.update(grad, state, model)\n",
    "    new_model = eqx.apply_updates(model, updates)\n",
    "    return new_model, new_state, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_t1</th>\n",
       "      <th>feature_t2</th>\n",
       "      <th>feature_t3</th>\n",
       "      <th>feature_t4</th>\n",
       "      <th>feature_t5</th>\n",
       "      <th>feature_t6</th>\n",
       "      <th>feature_t7</th>\n",
       "      <th>feature_t8</th>\n",
       "      <th>feature_t9</th>\n",
       "      <th>feature_t10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_t195</th>\n",
       "      <th>feature_t196</th>\n",
       "      <th>feature_t197</th>\n",
       "      <th>feature_t198</th>\n",
       "      <th>feature_t199</th>\n",
       "      <th>feature_t200</th>\n",
       "      <th>const_feature1</th>\n",
       "      <th>const_feature2</th>\n",
       "      <th>const_feature3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.316419</td>\n",
       "      <td>0.094006</td>\n",
       "      <td>0.180198</td>\n",
       "      <td>0.063395</td>\n",
       "      <td>-0.038635</td>\n",
       "      <td>-0.266741</td>\n",
       "      <td>0.463650</td>\n",
       "      <td>-0.027799</td>\n",
       "      <td>0.281768</td>\n",
       "      <td>0.130849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183962</td>\n",
       "      <td>0.830583</td>\n",
       "      <td>-0.634478</td>\n",
       "      <td>0.580287</td>\n",
       "      <td>1.022595</td>\n",
       "      <td>-0.281525</td>\n",
       "      <td>-0.274235</td>\n",
       "      <td>-0.193651</td>\n",
       "      <td>-0.583533</td>\n",
       "      <td>0.722456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.287647</td>\n",
       "      <td>-0.141836</td>\n",
       "      <td>-0.014514</td>\n",
       "      <td>-0.248675</td>\n",
       "      <td>0.633971</td>\n",
       "      <td>-0.472422</td>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.410364</td>\n",
       "      <td>0.127561</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428507</td>\n",
       "      <td>-0.645935</td>\n",
       "      <td>-0.264288</td>\n",
       "      <td>-0.563421</td>\n",
       "      <td>-0.999967</td>\n",
       "      <td>0.457197</td>\n",
       "      <td>0.161862</td>\n",
       "      <td>-0.130422</td>\n",
       "      <td>0.334030</td>\n",
       "      <td>0.218963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.431424</td>\n",
       "      <td>-0.110790</td>\n",
       "      <td>-0.125460</td>\n",
       "      <td>0.209351</td>\n",
       "      <td>-0.083805</td>\n",
       "      <td>-0.112344</td>\n",
       "      <td>0.197484</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>-0.077888</td>\n",
       "      <td>-0.127886</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.034519</td>\n",
       "      <td>-0.627236</td>\n",
       "      <td>-0.804753</td>\n",
       "      <td>-0.804350</td>\n",
       "      <td>-0.590270</td>\n",
       "      <td>-0.744307</td>\n",
       "      <td>-1.062656</td>\n",
       "      <td>-0.508828</td>\n",
       "      <td>0.316776</td>\n",
       "      <td>-0.856118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.091222</td>\n",
       "      <td>-0.278331</td>\n",
       "      <td>-0.173858</td>\n",
       "      <td>0.181299</td>\n",
       "      <td>-0.160004</td>\n",
       "      <td>0.034044</td>\n",
       "      <td>-0.264407</td>\n",
       "      <td>0.028212</td>\n",
       "      <td>-0.001304</td>\n",
       "      <td>-0.235955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989921</td>\n",
       "      <td>0.925385</td>\n",
       "      <td>-0.800712</td>\n",
       "      <td>0.223292</td>\n",
       "      <td>-0.795376</td>\n",
       "      <td>-0.598333</td>\n",
       "      <td>0.241152</td>\n",
       "      <td>0.732813</td>\n",
       "      <td>-0.329903</td>\n",
       "      <td>-0.398819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.168778</td>\n",
       "      <td>0.210203</td>\n",
       "      <td>-0.084391</td>\n",
       "      <td>-0.261193</td>\n",
       "      <td>0.218225</td>\n",
       "      <td>-0.118044</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>0.172762</td>\n",
       "      <td>-0.115775</td>\n",
       "      <td>0.083989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528696</td>\n",
       "      <td>-0.916553</td>\n",
       "      <td>0.057136</td>\n",
       "      <td>-0.111954</td>\n",
       "      <td>0.328855</td>\n",
       "      <td>0.925494</td>\n",
       "      <td>0.171832</td>\n",
       "      <td>-1.219297</td>\n",
       "      <td>-0.579278</td>\n",
       "      <td>-1.254390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_t1  feature_t2  feature_t3  feature_t4  feature_t5  feature_t6  \\\n",
       "0    0.316419    0.094006    0.180198    0.063395   -0.038635   -0.266741   \n",
       "1    0.287647   -0.141836   -0.014514   -0.248675    0.633971   -0.472422   \n",
       "2    0.431424   -0.110790   -0.125460    0.209351   -0.083805   -0.112344   \n",
       "3   -0.091222   -0.278331   -0.173858    0.181299   -0.160004    0.034044   \n",
       "4   -0.168778    0.210203   -0.084391   -0.261193    0.218225   -0.118044   \n",
       "\n",
       "   feature_t7  feature_t8  feature_t9  feature_t10  ...  feature_t195  \\\n",
       "0    0.463650   -0.027799    0.281768     0.130849  ...     -0.183962   \n",
       "1    0.048249    0.410364    0.127561     0.005904  ...      0.428507   \n",
       "2    0.197484    0.016432   -0.077888    -0.127886  ...     -1.034519   \n",
       "3   -0.264407    0.028212   -0.001304    -0.235955  ...     -0.989921   \n",
       "4   -0.007894    0.172762   -0.115775     0.083989  ...      0.528696   \n",
       "\n",
       "   feature_t196  feature_t197  feature_t198  feature_t199  feature_t200  \\\n",
       "0      0.830583     -0.634478      0.580287      1.022595     -0.281525   \n",
       "1     -0.645935     -0.264288     -0.563421     -0.999967      0.457197   \n",
       "2     -0.627236     -0.804753     -0.804350     -0.590270     -0.744307   \n",
       "3      0.925385     -0.800712      0.223292     -0.795376     -0.598333   \n",
       "4     -0.916553      0.057136     -0.111954      0.328855      0.925494   \n",
       "\n",
       "   const_feature1  const_feature2  const_feature3    target  \n",
       "0       -0.274235       -0.193651       -0.583533  0.722456  \n",
       "1        0.161862       -0.130422        0.334030  0.218963  \n",
       "2       -1.062656       -0.508828        0.316776 -0.856118  \n",
       "3        0.241152        0.732813       -0.329903 -0.398819  \n",
       "4        0.171832       -1.219297       -0.579278 -1.254390  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configurações\n",
    "n_samples = 1000  # número de amostras\n",
    "time_steps = 40   # passos temporais (para a RNN)\n",
    "n_features = 5    # número de features para a RNN\n",
    "n_const_features = 3  # número de features constantes para a MLP\n",
    "noise_level = 0.2  # nível de ruído\n",
    "\n",
    "# Função para gerar sinal senoidal com frequência variável\n",
    "def generate_sinusoidal_signal(time_steps, freq_base, freq_variation, noise_level):\n",
    "    time = np.arange(time_steps)\n",
    "    frequency = freq_base + np.random.uniform(-freq_variation, freq_variation)\n",
    "    signal = np.sin(2 * np.pi * frequency * time / time_steps)  # Gera sinal senoidal\n",
    "    noise = np.random.randn(time_steps) * noise_level  # Adiciona ruído\n",
    "    return signal + noise\n",
    "\n",
    "# Gerar série temporal com sinal senoidal para cada amostra e cada feature\n",
    "temporal_data = np.zeros((n_samples, time_steps, n_features))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for j in range(n_features):\n",
    "        freq_base = np.random.uniform(0.1, 1.0)  # Frequência base aleatória\n",
    "        freq_variation = 0.05  # Variação da frequência\n",
    "        temporal_data[i, :, j] = generate_sinusoidal_signal(time_steps, freq_base, freq_variation, noise_level)\n",
    "\n",
    "# Gerar features constantes para a MLP (randomizadas)\n",
    "const_data = np.random.randn(n_samples, n_const_features)\n",
    "\n",
    "# Combinar tudo em um dataframe\n",
    "df_temporal = pd.DataFrame(temporal_data.reshape(n_samples, -1),\n",
    "                           columns=[f'feature_t{i}' for i in range(1, time_steps * n_features + 1)])\n",
    "df_const = pd.DataFrame(const_data, columns=[f'const_feature{i}' for i in range(1, n_const_features + 1)])\n",
    "\n",
    "# Combinar features temporais e constantes\n",
    "df = pd.concat([df_temporal, df_const], axis=1)\n",
    "\n",
    "# Exemplo de target (valor a ser previsto pela RNN)\n",
    "df['target'] = np.random.randn(n_samples)\n",
    "\n",
    "(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
